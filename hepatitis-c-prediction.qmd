---
title: "Data Mining Final Project"
author: "RAVLEEN KAUR CHADHA, MRIDULA KALAISELVAN, YASH SHARMA"
format:
  html:
    embed-resources: true
toc: true
execute: 
  warning: false
  messages: false
---

```{r load-packages}

if(!require(pacman)) 
  install.packages("pacman") 

pacman::p_load(tidyverse, 
               scales, 
               devtools,
               here,
               class, 
               plotly,
               dplyr,
               caret,
               BiocManager,
               smotefamily,
               pROC, 
               mclust, 
               factoextra, 
               cluster, 
               dbscan,
               arules,
               arulesViz,
               themis,
               recipes,
               e1071
               )


```


```{r}
#| label: label-me-1
# Reading the dataset
hepatitis <- read_csv("HepatitisCdata.csv")
```


```{r}
#| label: label-me-2

# Handling the NA values by replacing them by the median of each column
hepatitis <- hepatitis %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))

# Verifying that all NA values have been handled
colSums(is.na(hepatitis))

write.csv(hepatitis, "HepatitisC_Cleaned.csv", row.names = FALSE)
```


```{r}
#| label: label-me-3

# EDA for the dataset

# Defining a colorblind-friendly palette

color_palette <- c("#440154FF", "#3B528BFF", "#21908CFF", "#5DC863FF", "#FDE725FF")

# Creating an enhanced interactive scatterplot

p <- plot_ly(
  data = hepatitis,
  x = ~AST, 
  y = ~ALT, 
  type = 'scatter', 
  mode = 'markers',
  color = ~as.factor(Category),
  colors = color_palette,
  marker = list(
    size = 8,
    opacity = 0.8
  ),
  text = ~paste(
    "Category:", case_when(
      Category == 0 ~ "Blood Donor",
      Category == 1 ~ "Suspect Blood Donor",
      Category == 2 ~ "Hepatitis",
      Category == 3 ~ "Fibrosis",
      Category == 4 ~ "Cirrhosis",
      TRUE ~ as.character(Category)
    ),
    "<br>AST:", AST,
    "<br>ALT:", ALT,
    "<br>Age:", Age
  )
) %>%
  layout(
    title = list(
      text = "Interactive Scatterplot: AST vs ALT by Category",
      font = list(size = 18)
    ),
    xaxis = list(
      title = "AST (Aspartate Transaminase)",
      titlefont = list(size = 14),
      zeroline = FALSE,
      showgrid = FALSE
    ),
    yaxis = list(
      title = "ALT (Alanine Transaminase)",
      titlefont = list(size = 14),
      zeroline = FALSE,
      showgrid = FALSE
    ),
    legend = list(
      title = list(text = "Category"),
      font = list(size = 12),
      orientation = "v",
      x = 1.1,
      y = 0.5,
      xanchor = "left"
    ),
    plot_bgcolor = "#FFFFFF",
    paper_bgcolor = "#FFFFFF"
  )

# Updating legend labels after plot creation

p <- p %>% layout(
  legend = list(
    title = list(text = "Category"),
    font = list(size = 12),
    orientation = "v",
    x = 1.1,
    y = 0.5,
    xanchor = "left",
    traceorder = "normal",
    itemsizing = "constant",
    labels = list(
      "0" = "Blood Donor",
      "1" = "Suspect Blood Donor",
      "2" = "Hepatitis",
      "3" = "Fibrosis",
      "4" = "Cirrhosis"
    )
  )
)

# Displaying the plot
p

```

### Classification Analysis for the dataset Hepatitis-C - Logistic Regression

```{r}
#| label: label-me-4

# Load the dataset
hepatitis <- read.csv("HepatitisC_Cleaned.csv")

# Convert 'Category' into a binary variable (e.g., "Blood Donor" vs. others)
hepatitis <- hepatitis %>%
  mutate(BinaryCategory = ifelse(Category == "0=Blood Donor", 1, 0))

# Encode 'Sex' as a numeric variable (e.g., "m" -> 1, "f" -> 0)
hepatitis <- hepatitis %>%
  mutate(Sex = ifelse(Sex == "m", 1, 0))

# Exclude unnecessary columns
hepatitis <- hepatitis %>%
  select(-...1, -Category)  # Remove index and original 'Category'

# Ensure the target variable is a factor
hepatitis$BinaryCategory <- as.factor(hepatitis$BinaryCategory)

# Split the dataset into training and testing sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(hepatitis$BinaryCategory, p = 0.8, list = FALSE)
train_data <- hepatitis[train_index, ]
test_data <- hepatitis[-train_index, ]

# Create a recipe for SMOTE
smote_recipe <- recipe(BinaryCategory ~ ., data = train_data) %>%
  step_smote(BinaryCategory, over_ratio = 1)  # Balance classes to a 1:1 ratio

# Prepare the SMOTE dataset
smote_data <- prep(smote_recipe) %>%
  bake(new_data = NULL)

# Check the class distribution after SMOTE
print("Class distribution after applying SMOTE:")
print(table(smote_data$BinaryCategory))

# Train logistic regression on the SMOTE-balanced dataset
smote_logistic_model <- glm(BinaryCategory ~ ., data = smote_data, family = "binomial")

# Summarize the model
print("Logistic Regression Model Summary:")
print(summary(smote_logistic_model))

# Predict on the test set
smote_predictions <- predict(smote_logistic_model, newdata = test_data, type = "response")

# Convert probabilities to class labels (threshold = 0.5)
smote_predicted_classes <- ifelse(smote_predictions > 0.5, 1, 0)

# Evaluate the model
smote_confusion_matrix <- confusionMatrix(as.factor(smote_predicted_classes), as.factor(test_data$BinaryCategory))
print("Confusion Matrix after applying SMOTE:")
print(smote_confusion_matrix)

# Plot ROC curve for the SMOTE model
smote_roc_curve <- roc(as.numeric(test_data$BinaryCategory), smote_predictions)
plot(smote_roc_curve, col = "red", main = "ROC Curve for Logistic Regression (SMOTE)")

```

### Probability Distribution Visualization for Logistic Regression

```{r}
#| label: label-me-5
test_data$Predicted_Probabilities <- smote_predictions
ggplot(test_data, aes(x = Predicted_Probabilities, fill = BinaryCategory)) +
  geom_density(alpha = 0.5) +
  labs(title = "Probability Distribution for Logistic Regression",
       x = "Predicted Probability",
       y = "Density",
       fill = "Actual Class") +
  theme_minimal()
```

### GMM Clustering 

```{r}
#| label: label-me-8
# Step 1: Load the dataset
# Assuming the data has been preprocessed (scaled, PCA applied, and features weighted)
hepatitis_data <- read.csv("HepatitisC_Cleaned.csv")

# Select numeric columns for clustering
numeric_data <- hepatitis_data[sapply(hepatitis_data, is.numeric)]
scaled_data <- scale(numeric_data)

# Perform PCA for dimensionality reduction
pca <- prcomp(scaled_data, center = TRUE, scale. = TRUE)
pca_data <- pca$x[, 1:2]  # Retain the first two principal components

# Step 2: Apply Gaussian Mixture Models (GMM)
set.seed(123)  # Ensure reproducibility
gmm_model <- Mclust(pca_data)

# Optimal number of clusters
cat("Optimal Number of Clusters (GMM):", gmm_model$G, "\n")

# Step 3: Evaluate Clustering Quality
# Silhouette Score
silhouette_scores <- silhouette(gmm_model$classification, dist(pca_data))
avg_silhouette <- mean(silhouette_scores[, 3])
cat("Average Silhouette Score (GMM):", round(avg_silhouette, 2), "\n")

# Adjusted Rand Index (ARI)
if ("Category" %in% colnames(hepatitis_data)) {
  actual_labels <- as.numeric(factor(hepatitis_data$Category))  # Replace 'Category' with your actual label column
  ari <- adjustedRandIndex(actual_labels, gmm_model$classification)
  cat("Adjusted Rand Index (ARI):", round(ari, 2), "\n")
} else {
  cat("No actual labels provided for ARI calculation.\n")
}

# Step 5: Add Cluster Assignments to Dataset
hepatitis_data$Cluster <- as.factor(gmm_model$classification)

# Step 6: Analyze Clusters
# Summary statistics for each cluster
cluster_summary <- hepatitis_data %>%
  group_by(Cluster) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))

# Enhanced GMM Clustering Visualization with Labels
ggplot(as.data.frame(pca_data), aes(x = PC1, y = PC2, color = as.factor(gmm_model$classification))) +
  geom_point(size = 3, alpha = 0.7) +  # Data points
  stat_ellipse(aes(fill = as.factor(gmm_model$classification)), alpha = 0.2, geom = "polygon") +  # Ellipses
  scale_color_manual(values = c("#4CAF50", "#FF5722"), name = "Cluster",
                     labels = c("Cluster 1: Healthy/Low Risk", "Cluster 2: At-Risk/Diseased")) +
  scale_fill_manual(values = c("#4CAF50", "#FF5722"), name = "Cluster",
                    labels = c("Cluster 1: Healthy/Low Risk", "Cluster 2: At-Risk/Diseased")) +
  labs(
    title = "Gaussian Mixture Model Clustering",
    subtitle = "Principal Components and Identified Groups",
    x = "Principal Component 1",
    y = "Principal Component 2"
  ) +
  theme_minimal(base_size = 15) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5),
    legend.position = "right",
    legend.title = element_text(face = "bold"),
    legend.text = element_text(size = 10), 
    axis.title.x = element_text(size = 10), 
    axis.title.y = element_text(size = 10)
  )

```


```{r}
#| label: label-me-9

# Scale the data
scaled_data <- scale(pca_data)

# Apply DBSCAN with eps = 1.0 and minPts = 5 (rule of thumb: dimensions + 1)
dbscan_model <- dbscan(scaled_data, eps = 1, minPts = 5)

# Calculate cluster sizes 
cluster_sizes <- table(dbscan_model$cluster)

# Calculate the percentage of noise
noise_percentage <- sum(dbscan_model$cluster == 0) / nrow(scaled_data) * 100

# Compute silhouette scores
silhouette_scores <- silhouette(dbscan_model$cluster, dist(scaled_data))
avg_silhouette <- mean(silhouette_scores[, 3])

# Visualize DBSCAN clusters
ggplot(as.data.frame(pca_data), aes(x = PC1, y = PC2, color = as.factor(dbscan_model$cluster))) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_manual(
    values = c("#999999", "#E41A1C", "#377EB8", "#4DAF4A", "#FF7F00"),
    name = "Cluster",
    labels = c("Noise", "Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4")
  ) +
  labs(
    title = "DBSCAN Clustering Results",
    x = "Principal Component 1",
    y = "Principal Component 2"
  ) +
  theme_minimal(base_size = 15) +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.position = "right",
    legend.text = element_text(size = 10), 
    axis.title.x = element_text(size = 10), 
    axis.title.y = element_text(size = 10)
  )

cat("DBSCAN Results Summary:\n")
cat("Number of Clusters:", length(unique(dbscan_model$cluster)) - 1, "\n")  # Exclude noise cluster
cat("Noise Percentage:", round(noise_percentage, 2), "%\n")
cat("Average Silhouette Score:", round(avg_silhouette, 2), "\n")
cat("\nAdditional Insights:\n")
cat("- Noise (Cluster 0) contains", cluster_sizes[1], "points.\n")
cat("- Largest cluster size:", max(cluster_sizes[-1]), "points (excluding noise).\n")
cat("- Smallest cluster size:", min(cluster_sizes[-1]), "points (excluding noise).\n")

```


### Classification Analysis for the dataset Hepatitis-C - SVM

```{r}
#| label: label-me-10

# Load the dataset
hepatitis <- read.csv("HepatitisC_Cleaned.csv")

# Preprocessing
# Encode 'Sex' as a factor
hepatitis$Sex <- as.factor(hepatitis$Sex)

# Convert 'Category' to a factor for multi-class classification
hepatitis$Category <- as.factor(hepatitis$Category)

# Split the data into training and testing sets
set.seed(123)  # For reproducibility
trainIndex <- createDataPartition(hepatitis$Category, p = 0.8, list = FALSE)
trainData <- hepatitis[trainIndex, ]
testData <- hepatitis[-trainIndex, ]

# Identify numeric features
numeric_features <- names(which(sapply(trainData, is.numeric)))

# Scale numeric features
scaled_train <- trainData
scaled_test <- testData

for (col in numeric_features) {
  # Get scaling parameters from the training set
  col_mean <- mean(trainData[[col]], na.rm = TRUE)
  col_sd <- sd(trainData[[col]], na.rm = TRUE)
  
  # Scale training and test data
  scaled_train[[col]] <- (trainData[[col]] - col_mean) / col_sd
  scaled_test[[col]] <- (testData[[col]] - col_mean) / col_sd
}

# Train the SVM model with a radial basis function kernel
svm_model <- svm(Category ~ ., data = scaled_train, kernel = "radial", cost = 1, gamma = 0.1)

# Predict on the test set
svm_predictions <- predict(svm_model, newdata = scaled_test)

# Evaluate the model
conf_matrix <- confusionMatrix(svm_predictions, scaled_test$Category)
print("Confusion Matrix for SVM:")
print(conf_matrix)

# Install and load the GGally package if not already installed
if (!requireNamespace("GGally", quietly = TRUE)) {
  install.packages("GGally")
}
library(GGally)

# Specify the features to include in the plot
selected_features <- c("Age", "ALB", "ALP", "ALT", "AST", "BIL")

# Filter the dataset for specific categories
filtered_data <- scaled_train %>%
  filter(Category %in% c("1=Hepatitis", "2=Fibrosis", "3=Cirrhosis"))  # Assuming "1" = Hepatitis, "2" = Fibrosis, "3" = Cirrhosis

# Parallel coordinates plot for selected features and categories
ggparcoord(data = filtered_data,
           columns = which(names(filtered_data) %in% selected_features),
           groupColumn = "Category",
           scale = "std", alphaLines = 0.5) +
  labs(title = "Parallel Coordinates Plot for Hepatitis, Fibrosis, and Cirrhosis",
       x = "Features", y = "Scaled Values") +
  theme_minimal(base_size = 14)
```
### Association rule using Apriori Algorithm

```{r }

# 1. Data Preparation
hepatitis_categorized <- hepatitis %>%
  mutate(
    Age_Group = case_when(
      Age < 35 ~ "Young",
      Age < 50 ~ "Middle",
      Age < 65 ~ "Senior",
      TRUE ~ "Elderly"
    ),
    ALT_Level = case_when(
      ALT < 40 ~ "Normal",
      ALT < 80 ~ "Mild_Elevation",
      ALT < 200 ~ "Moderate_Elevation",
      TRUE ~ "Severe_Elevation"
    ),
    AST_Level = case_when(
      AST < 40 ~ "Normal",
      AST < 80 ~ "Mild_Elevation",
      AST < 200 ~ "Moderate_Elevation",
      TRUE ~ "Severe_Elevation"
    ),
    Disease_Status = case_when(
      Category == 0 ~ "Blood_Donor",
      Category == 1 ~ "Suspect",
      Category == 2 ~ "Hepatitis",
      Category == 3 ~ "Fibrosis",
      Category == 4 ~ "Cirrhosis"
    )
  )

# 2. Create transactions
hepatitis_trans <-
  as(data.frame(select(hepatitis_categorized, 
    Age_Group, ALT_Level, AST_Level, Disease_Status)), 
    "transactions")

# 3. Apply Apriori algorithm
rules <- apriori(hepatitis_trans,
    parameter = list(
        support = 0.03,
        confidence = 0.5,
        minlen = 2,
        maxlen = 4
    ))

# 4. Process rules
rules_pruned <- 
  rules[!is.redundant(rules)]
rules_sorted <- 
  sort(rules_pruned, by = "lift", decreasing = TRUE)

# 5. Create rules dataframe
rules_df <- data.frame(
    lhs = labels(lhs(rules_sorted)),
    rhs = labels(rhs(rules_sorted)),
    support = quality(rules_sorted)$support,
    confidence = quality(rules_sorted)$confidence,
    lift = quality(rules_sorted)$lift
)

# 6. Visualization
# Scatter Plot
scatter_plot <- plot_ly(rules_df, 
        x = ~support,
        y = ~confidence,
        color = ~lift,
        type = "scatter",
        mode = "markers",
        marker = list(size = 10),
        text = ~paste("Rule:", lhs, "=>", rhs)) %>%
    layout(
        title = "Apriori Algorithm",
        xaxis = list(title = "Support"),
        yaxis = list(title = "Confidence"),
        colorbar = list(title = "Lift")
    )



# 7. Print Quality Assessment
cat("\nAssociation Rule Analysis Summary:")
cat("\nNumber of Rules:", nrow(rules_df))
cat("\nAverage Confidence (Accuracy):", round(mean(rules_df$confidence), 4))
cat("\nAverage Lift:", round(mean(rules_df$lift), 4))
cat("\nAverage Support:", round(mean(rules_df$support), 4))

# Print top 10 rules
cat("\n\nTop 10 Rules by Lift:\n")
head(rules_df[order(-rules_df$lift), 
              c("lhs", "rhs", "support", "confidence", "lift")], 10)

# Display plots
scatter_plot



```

### CHARM Algorithm 

```{r}
#| label: label-me-11

# 1. Data Preprocessing
hepatitis_categorized <- hepatitis %>%
  mutate(
    Age_Group = case_when(
      Age < 35 ~ "Young",
      Age < 50 ~ "Middle",
      Age < 65 ~ "Senior",
      TRUE ~ "Elderly"
    ),
    ALT_Level = case_when(
      ALT < 40 ~ "ALT_Normal",
      ALT < 80 ~ "ALT_Mild_Elevation",
      ALT < 200 ~ "ALT_Moderate_Elevation",
      TRUE ~ "ALT_Severe_Elevation"
    ),
    AST_Level = case_when(
      AST < 40 ~ " AST_Normal",
      AST < 80 ~ " AST_Mild_Elevation",
      AST < 200 ~ " AST_Moderate_Elevation",
      TRUE ~ " AST_Severe_Elevation"
    ),
    Disease_Status = case_when(
      Category == 0 ~ "Blood_Donor",
      Category == 1 ~ "Suspect",
      Category == 2 ~ "Hepatitis",
      Category == 3 ~ "Fibrosis",
      Category == 4 ~ "Cirrhosis"
    )
  )

# 2. CHARM Implementation
charm_association_rules <- function(data, min_support = 0.05, min_confidence = 0.5) {
    # Convert to transaction format
    trans_matrix <- as.matrix(data)
    n_trans <- nrow(trans_matrix)
    
    # Find unique items
    items <- unique(as.character(trans_matrix))
    cat("Initial items:", length(items), "\n")
    
    # Create vertical database (tid-lists)
    tid_lists <- list()
    frequent_items <- character()
    
    # Find frequent single items
    for(item in items) {
        tids <- which(apply(trans_matrix, 1, function(x) item %in% x))
        support <- length(tids)/n_trans
        if(support >= min_support) {
            tid_lists[[item]] <- tids
            frequent_items <- c(frequent_items, item)
        }
    }
    
    cat("Frequent items:", length(frequent_items), "\n")
    
    # Store closed itemsets
    closed_itemsets <- list()
    
    # Find closed itemsets
    find_closed_itemsets <- function(prefix = character(), prefix_tids = NULL, 
                                   remaining_items = frequent_items) {
        for(i in seq_along(remaining_items)) {
            item <- remaining_items[i]
            new_tids <- if(is.null(prefix_tids)) tid_lists[[item]]
                       else intersect(prefix_tids, tid_lists[[item]])
            
            support <- length(new_tids)/n_trans
            
            if(support >= min_support) {
                new_itemset <- c(prefix, item)
                
                # Check if closed
                is_closed <- TRUE
                for(existing in names(closed_itemsets)) {
                    if(all(new_itemset %in% strsplit(existing, ",")[[1]]) && 
                       closed_itemsets[[existing]]$support == support) {
                        is_closed <- FALSE
                        break
                    }
                }
                
                if(is_closed) {
                    closed_itemsets[[paste(new_itemset, collapse=",")]] <<- list(
                        items = new_itemset,
                        support = support,
                        tids = new_tids
                    )
                }
                
                # Recursive call with remaining items
                if(i < length(remaining_items)) {
                    find_closed_itemsets(new_itemset, new_tids, 
                                       remaining_items[(i+1):length(remaining_items)])
                }
            }
        }
    }
    
    # Generate closed itemsets
    cat("Generating closed itemsets...\n")
    find_closed_itemsets()
    cat("Found", length(closed_itemsets), "closed itemsets\n")
    
    # Generate rules from closed itemsets
    rules <- data.frame(
        lhs = character(),
        rhs = character(),
        support = numeric(),
        confidence = numeric(),
        lift = numeric(),
        stringsAsFactors = FALSE
    )
    
    # Generate rules
    cat("Generating association rules...\n")
    for(itemset_key in names(closed_itemsets)) {
        itemset <- closed_itemsets[[itemset_key]]
        if(length(itemset$items) >= 2) {
            for(i in 1:(length(itemset$items)-1)) {
                combs <- combn(itemset$items, i, simplify = FALSE)
                for(lhs in combs) {
                    rhs <- setdiff(itemset$items, lhs)
                    
                    # Calculate confidence
                    lhs_support <- max(sapply(names(closed_itemsets), function(key) {
                        if(all(lhs %in% closed_itemsets[[key]]$items))
                            return(closed_itemsets[[key]]$support)
                        return(0)
                    }))
                    
                    if(lhs_support > 0) {
                        confidence <- itemset$support / lhs_support
                        
                        if(confidence >= min_confidence) {
                            # Calculate lift
                            rhs_support <- max(sapply(names(closed_itemsets), function(key) {
                                if(all(rhs %in% closed_itemsets[[key]]$items))
                                    return(closed_itemsets[[key]]$support)
                                return(0)
                            }))
                            
                            lift <- confidence / rhs_support
                            
                            rules[nrow(rules) + 1,] <- list(
                                lhs = paste(lhs, collapse=", "),
                                rhs = paste(rhs, collapse=", "),
                                support = itemset$support,
                                confidence = confidence,
                                lift = lift
                            )
                        }
                    }
                }
            }
        }
    }
    
    return(rules)
}

# 3. Apply CHARM to dataset
selected_data <- select(hepatitis_categorized, 
                       Age_Group, ALT_Level, AST_Level, Disease_Status)
rules <- charm_association_rules(selected_data, min_support = 0.05, min_confidence = 0.5)

# 4. Visualize results
if(nrow(rules) > 0) {
    # Scatter plot
    scatter_plot <- plot_ly(rules,
        x = ~confidence,
        y = ~lift,
        color = ~support,
        type = "scatter",
        mode = "markers",
        text = ~paste("Rule:", lhs, "=>", rhs,
                     "<br>Support:", round(support, 4),
                     "<br>Confidence:", round(confidence, 4),
                     "<br>Lift:", round(lift, 4))
    ) %>%
        layout(
            title = "CHARM Algotrithm",
            xaxis = list(title = "Confidence"),
            yaxis = list(title = "Lift"),
            colorbar = list(title = "Support")
        )
    
    # Print statistics
    cat("\nAssociation Rule Analysis Results:\n")
    cat("Total rules found:", nrow(rules), "\n")
    cat("Average confidence:", round(mean(rules$confidence), 4), "\n")
    cat("Average lift:", round(mean(rules$lift), 4), "\n")
    cat("Average support:", round(mean(rules$support), 4), "\n")
    
    # Print top rules
    cat("\nTop 10 Rules by Lift:\n")
    print(head(rules[order(-rules$lift), ], 10))
    
    # Save results
    write.csv(rules, "charm_rules.csv", row.names = FALSE)
} else {
    cat("No rules found. Try adjusting the minimum support or confidence thresholds.\n")
}

    # Display plot
  scatter_plot
```


